{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80db7e2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sprs\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "from imblearn.under_sampling import TomekLinks\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0f986c3",
   "metadata": {},
   "source": [
    "## 1 Introduction to Natural Language Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "66d8ccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataset\n",
    "df = pd.read_csv(\"IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "673455ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to remove html tags\n",
    "def remove_html(data):\n",
    "    return re.sub(r'<.*?>',' ', data)\n",
    "\n",
    "#function to remove url\n",
    "def remove_url(data):\n",
    "    return re.sub(r'https?://\\S+', ' ', data)\n",
    "\n",
    "#remove punctuation\n",
    "def remove_punc(data):\n",
    "    return re.sub('\\W+','', data)\n",
    "\n",
    "#main function\n",
    "stop_words = stopwords.words('english')\n",
    "def preprocess_text(data):\n",
    "    clear_data = remove_html(remove_url(data.lower())).split()\n",
    "    #this remove emails\n",
    "    text = (' '.join([remove_punc(w) for w in clear_data if not w in stop_words and \"@\" not in w]))\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6addaead",
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning the reviews from html tags, urls, stopwords and punctutation\n",
    "df['review'] = df['review'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "55cc11aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NLPModels:\n",
    "    def __init__(self, col):\n",
    "        self.col = col #column in which text data is stored\n",
    "        \n",
    "    def fit(self):\n",
    "        self.count_words_across_document = {} #this is used to track the number of comments which contain a particular word\n",
    "        self.list_words_sentences = [] #this list tracks word counts of each comment\n",
    "        for sent in self.col: #iterating over each comment\n",
    "            count_in_sentence = {} #this tracks the word seen in comment so far and its count\n",
    "            for word in sent.split(): #iterating over each word in a comment\n",
    "                \n",
    "                #if word is already in dictionary increase the count else make a new key\n",
    "                if word in self.count_words_across_document:\n",
    "                    self.count_words_across_document[word]+=1\n",
    "                    \n",
    "                elif word not in self.count_words_across_document:\n",
    "                    self.count_words_across_document[word]=1\n",
    "                \n",
    "                #this tracks word in each comment only\n",
    "                if word in count_in_sentence:\n",
    "                    count_in_sentence[word]+=1\n",
    "\n",
    "                elif word not in count_in_sentence:\n",
    "                    count_in_sentence[word]=1\n",
    "            \n",
    "            #make a list containing all the words across all comments\n",
    "            self.list_words_sentences.append(count_in_sentence)\n",
    "            \n",
    "        #to make a two way connection between going from words to index or vice versa\n",
    "        self.idx_to_word= {i:w for i,w in enumerate(self.count_words_across_document)}\n",
    "        self.word_to_idx = {w:i for i,w in enumerate(self.count_words_across_document)}\n",
    "        self.vocab_size = len(self.count_words_across_document) #number of unique words\n",
    "            \n",
    "    def BOW(self):\n",
    "        bow = sprs.lil_matrix((1,self.vocab_size)) #sparse matrix as they are more efficent\n",
    "        for sent in self.list_words_sentences: #iterate over each comment words dictionary\n",
    "            rep = sprs.lil_matrix((1,self.vocab_size)) #empty matrix to store the final representation of the word\n",
    "            for word, count in sent.items(): #iterating ober each word\n",
    "                rep[0,self.word_to_idx[word]]=count #replacing the count in the representation matrix of that word\n",
    "            bow = sprs.vstack([bow,rep]) #concatenate the matrix vertically to obtain one big matrix of size mxd\n",
    "        return bow.toarray()[1:,:] \n",
    "    \n",
    "    def TF_IDF(self):\n",
    "        tf_idf = sprs.lil_matrix((1,self.vocab_size)) #sparse matrix as they are more efficent\n",
    "        for sent in self.list_words_sentences: #iterate over each comment words dictionary\n",
    "            rep = sprs.lil_matrix((1,self.vocab_size)) #empty matrix to store the final representation of the word\n",
    "            for word, count in sent.items(): #iterating ober each word\n",
    "                #term frequency-inverse document frequency\n",
    "                rep[0,self.word_to_idx[word]]=count/len(sent) * np.log(len(self.col)/self.count_words_across_document[word])\n",
    "            tf_idf = sprs.vstack([tf_idf,rep]) #concatenate the matrix vertically to obtain one big matrix of size mxd\n",
    "        return tf_idf.toarray()[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "7dd4a393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "n = NLPModels(df['review'].iloc[:100]) \n",
    "#fit the model\n",
    "n.fit()\n",
    "#obtain the BOW representation\n",
    "bow_representation = n.BOW()\n",
    "print(bow_representation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "83982c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.00014246  0.03313072  0.02315738 ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.00024753  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.00023859  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [-0.00030004  0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.02436598  0.02436598\n",
      "   0.02436598]]\n"
     ]
    }
   ],
   "source": [
    "#obtain the tfidf representation\n",
    "tf_idf_representation = n.TF_IDF()\n",
    "print(tf_idf_representation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d29f103",
   "metadata": {},
   "source": [
    "## 2 Support Vector Machines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be41f8c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the data\n",
    "df = pd.read_csv(\"creditcard.csv\")\n",
    "#separating into X and Y\n",
    "X = df.iloc[:,:-1]\n",
    "y=df.iloc[:,-1]\n",
    "y = df.iloc[:,-1].values\n",
    "#make the y labels as -1,1 instead of 0,1\n",
    "y = np.where(y>0,y,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d2add4d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10b63f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "over_samp = RandomOverSampler(random_state=42) #oversampling\n",
    "under_samp = TomekLinks() #undersampling\n",
    "smote = SMOTE(random_state=101) #SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7ba9ae38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pegasos:\n",
    "    def __init__ (self, lamda, k, projection):\n",
    "        self.lamda = lamda #lambda value\n",
    "        self.k = k #number of observations to be used\n",
    "        self.projection = projection #for projections\n",
    "        \n",
    "    def gradient(self, p): #to calculate the gradient\n",
    "        return np.where(p<1,1,0)\n",
    "    \n",
    "    def fit(self, x, y, n_iters=1500):\n",
    "        m, n = x.shape\n",
    "        self.W = np.zeros(n)\n",
    "        for t in range(1,n_iters+1): #iterate till max_iters\n",
    "            #pick a random instance\n",
    "            idx = np.random.choice(range(m), self.k, replace=False)\n",
    "            lr = 1/(self.lamda*t) #get the learning rate\n",
    "            x_i = x[idx] #get x_i\n",
    "            y_i = y[idx] #get y_i\n",
    "            prod = y_i * (x_i@self.W) #obtain the product\n",
    "            #update the weights\n",
    "            self.W = (1-lr*self.lamda)*self.W + \\\n",
    "            (lr/self.k)*(np.sum(np.multiply(y_i.reshape(-1,1),x_i)*self.gradient(prod).reshape(-1,1),axis=0))\n",
    "    \n",
    "    def predict(self, x):\n",
    "        #transform the inputs using the weight vector\n",
    "        p = x@self.W.reshape(-1,1)\n",
    "        return np.sign(p) #the sign function outputs the class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "78c574aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVMDC:\n",
    "    def __init__ (self, C, mode = \"L1\", tol=1e-3):\n",
    "        self.C = C #C value\n",
    "        self.mode = mode\n",
    "        self.tol = tol #tolerance value to break out of the loop\n",
    "    \n",
    "    def partial_gradient(self,G,a,U): #to calculate the partial gradient\n",
    "        if a == 0:\n",
    "            return min(G,0)\n",
    "        elif a == U:\n",
    "            return max(G,0)\n",
    "        elif (a>0) and (a<U):\n",
    "            return G\n",
    "    \n",
    "    def fit(self, X, y,iters=100):\n",
    "        m, n = X.shape\n",
    "        self.w = 0 #weight matrix\n",
    "        \n",
    "        #SVMDC can be done in L1 and L2 modes\n",
    "        if self.mode == \"L1\":\n",
    "            Dii = 0\n",
    "            U=self.C\n",
    "        else:\n",
    "            Dii = 1/(2*self.C)\n",
    "            U=np.inf\n",
    "        \n",
    "        #to get the langrangian multipliers\n",
    "        alpha = np.zeros(m)\n",
    "        self.w = np.zeros(shape=(n)) #initialize the weight matrix\n",
    "        Qii = np.sum(X**2, 1) + Dii #calculate Qii\n",
    "        for t in range(iters): #iterate till max_iters\n",
    "            err = 0 #calculate error to break the loop\n",
    "            for i in range(m): #iterate over each instance\n",
    "                Qhat = Qii[i] #get Q_bar\n",
    "                G = np.multiply(np.dot(self.w,X[i,:]),y[i]) - 1 + Dii * alpha[i] #gradient of the objective function\n",
    "                PG = self.partial_gradient(G,alpha[i],U) #partial gradient of the objective function\n",
    "                if np.abs(G) > err: #to keep updating the error term\n",
    "                    err = np.abs(G)\n",
    "                \n",
    "                #to find optimal solution\n",
    "                if np.abs(G) > 0: \n",
    "                    alpha_new = min(max(alpha[i]-G/Qhat,0),U)\n",
    "                    self.w = self.w+(np.multiply((alpha_new - alpha[i])* y[i] ,X[i,:]))\n",
    "                    alpha[i] = alpha_new\n",
    "            \n",
    "            #stop iterating once the error fall below tolerance        \n",
    "            if err<self.tol:\n",
    "                break\n",
    "        \n",
    "    def predict(self, x):\n",
    "        #project the points using the weight matrix\n",
    "        p = x@self.w.reshape(-1,1)\n",
    "        return np.sign(p) #the sign function tells the class which the object belong to"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65548f23",
   "metadata": {},
   "source": [
    "Now Experiment with different sampling techniques on these algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49416206",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = [over_samp,under_samp,smote]\n",
    "peg_single = Pegasos(0.01,1,projection=False)\n",
    "peg_batch = Pegasos(0.01,10,projection=False)\n",
    "svm = SVMDC(0.1)\n",
    "algo = [peg_single, peg_batch, svm]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe612ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "entry = []\n",
    "for samp in sampler:\n",
    "    X_train_ov,y_train_ov = samp.fit_resample(X_train,y_train)\n",
    "    model_accuracy = []\n",
    "    for model in algo:\n",
    "        model.fit(X_train_ov.values,y_train_ov)\n",
    "        model_accuracy.append(accuracy_score(y_test,model.predict(X_test)))\n",
    "    entry.append(model_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c0d392a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.DataFrame(entry,index=['Pegasos','Pegasos Mini Batch','SVMDC'],columns=['Oversampler','Undersampler','SMOTE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "aed71093",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Oversampler</th>\n",
       "      <th>Undersampler</th>\n",
       "      <th>SMOTE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Pegasos</th>\n",
       "      <td>0.998192</td>\n",
       "      <td>0.998227</td>\n",
       "      <td>0.997928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pegasos Mini Batch</th>\n",
       "      <td>0.998280</td>\n",
       "      <td>0.998280</td>\n",
       "      <td>0.998244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVMDC</th>\n",
       "      <td>0.001720</td>\n",
       "      <td>0.998262</td>\n",
       "      <td>0.006144</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Oversampler  Undersampler     SMOTE\n",
       "Pegasos                0.998192      0.998227  0.997928\n",
       "Pegasos Mini Batch     0.998280      0.998280  0.998244\n",
       "SVMDC                  0.001720      0.998262  0.006144"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e02cd9e",
   "metadata": {},
   "source": [
    "**Conclusion:** As we can infer from the results that the accuracies are highly volatile. The random state provided in the sampler highly influences the accuracy of the model on the test set. It was also found that for SVMDC only undersampling gave good results, different random state was also experimented here but no good was found. Among different models, the results were found to be more or less similar, however pegasos mini batch was found to be performing slightly better than the rest on all sampler. In the end it is also recommended to prefer undersampling as it keeps the size of the data manageable and avoid long training times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd4408",
   "metadata": {},
   "source": [
    "## 3. Optuna - A hyperparameter optimization framework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe87d897",
   "metadata": {},
   "source": [
    "Done on Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f6cb90",
   "metadata": {},
   "source": [
    "References:\n",
    "1. https://medium.com/@jorlugaqui/how-to-strip-html-tags-from-a-string-in-python-7cb81a2bbf44\n",
    "2. https://stackoverflow.com/questions/11331982/how-to-remove-any-url-within-a-string-in-python\n",
    "3. https://stackoverflow.com/questions/5843518/remove-all-special-characters-punctuation-and-spaces-from-string\n",
    "4. https://github.com/samehkhamis/SVMDC\n",
    "5. https://github.com/karndeepsingh/optuna"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
