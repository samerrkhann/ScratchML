{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import imageio\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import load_digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Perceptron Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## funtion for normalizing the features\n",
    "def normalize_features(X,append=True):\n",
    "    X = (X - np.mean(X, 0)) / np.std(X, 0) #normalize the features\n",
    "    if append:\n",
    "        X = np.append(np.ones(X.shape[0]).reshape(-1,1),X,1) #append column of ones for intercept\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron:\n",
    "    def __init__ (self, X, y, iterations=50):\n",
    "        self.X = X\n",
    "        self.y = y.reshape(-1,1)\n",
    "        self.m, self.n = self.X.shape\n",
    "        self.i = iterations #number of max iterations\n",
    "    \n",
    "    def fit(self, lr=0.1, tolerance=1e-3):\n",
    "        #initialize random weight vector\n",
    "        W = np.ones(shape=(self.n,1))\n",
    "        #to check if the updation becomes insignificant\n",
    "        old_W = W\n",
    "        #to keep track how the value changes\n",
    "        w_hist = []\n",
    "        \n",
    "        #iterating till max iters\n",
    "        for i in range(self.i):\n",
    "            \n",
    "            #forward pass\n",
    "            Z = self.X @ old_W\n",
    "            \n",
    "            #taking sigmoid\n",
    "            p = 1./(1+np.exp(-Z))\n",
    "            \n",
    "            #calculating loss\n",
    "            loss = (-1/self.m * (np.dot(self.y.T,np.log(p))+np.dot(np.transpose(1-self.y),np.log(1-p))))\n",
    "            \n",
    "            #calaculating gradient\n",
    "            g = -1/self.m * np.dot(self.X.T,(self.y-p))\n",
    "            \n",
    "            #updating value using gradient descent\n",
    "            new_W = old_W - lr*g\n",
    "\n",
    "            if np.all(np.abs(new_W - old_W) <= tolerance):\n",
    "                break\n",
    "            \n",
    "            #storing value of updated weights\n",
    "            w_hist.append(old_W)  \n",
    "            \n",
    "            #replacing old weights with new\n",
    "            old_W = new_W\n",
    "        \n",
    "        #storing final weights and history of weights for visualization purposes\n",
    "        self.W = old_W\n",
    "        self.hist = w_hist\n",
    "        \n",
    "    def predict(self,x):\n",
    "        scores = x @ self.W\n",
    "        return 1./(1+np.exp(-scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b09358",
   "metadata": {},
   "source": [
    "### 1. Linearly Separable Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdf399cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading the data\n",
    "with open('Xlin_sep.npy','rb') as f:\n",
    "    X = np.load(f)\n",
    "with open('ylin_sep.npy','rb') as f:\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "945a2b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to set lables as binary 0 1\n",
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c8bbbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=101)\n",
    "\n",
    "#normalize training set\n",
    "X_train_normalized = normalize_features(X_train)\n",
    "\n",
    "#normalizing test set\n",
    "X_test_normalized = normalize_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b92fc707",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the model\n",
    "p = Perceptron(X_train_normalized,y_train)\n",
    "#fitting the perceptron\n",
    "p.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e7f760",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get predictions on train and test set\n",
    "preds_train = p.predict(X_train_normalized)\n",
    "preds_test = p.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ee6da585",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on Train Set is 1.0\n",
      "The accuracy on Test Set is 1.0\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy on Train Set is',accuracy_score(y_train,preds_train>0.5))\n",
    "print('The accuracy on Test Set is',accuracy_score(y_test,preds_test>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9417dafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Visualize:\n",
    "    \n",
    "    def __init__(self, X, y, W):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.W = W\n",
    "    \n",
    "    def plot_decision_boundary_2D(self,w):\n",
    "        bias = w[0]\n",
    "        weights = w[1:]\n",
    "        bp = (-bias /(weights.T @ weights)) * weights\n",
    "        xx , yy = bp\n",
    "        m = yy/xx\n",
    "        m = -1/m\n",
    "        xp = np.linspace(-3,3,100)\n",
    "        yp = yy - m*xx +m*xp\n",
    "        return xp, yp\n",
    "    \n",
    "    def create_frame(self,t,w,name):\n",
    "    \n",
    "        if not os.path.exists(name):\n",
    "            os.mkdir(name)\n",
    "\n",
    "        fig = plt.figure(figsize=(6, 4))\n",
    "        xp,yp = self.plot_decision_boundary_2D(w)\n",
    "        plt.title(f'Decision Boundary of {name} at iteration {t}',fontsize = 14)\n",
    "        plt.plot(xp,yp,color='green')\n",
    "        scatter = plt.scatter(self.X[:,1],self.X[:,2],c=self.y,cmap = 'coolwarm', label=self.y)\n",
    "        plt.ylim(-3, 3)\n",
    "        classes = ['0','1']\n",
    "        plt.xlabel('Feature 1',fontsize = 14);\n",
    "        plt.ylabel('Feature 2',fontsize = 14);\n",
    "        plt.legend(handles=scatter.legend_elements()[0], labels=classes);\n",
    "        plt.savefig(f'./{name}/img_{t}.png', \n",
    "                    transparent = False,  \n",
    "                    facecolor = 'white'\n",
    "                   )\n",
    "        plt.close()\n",
    "        \n",
    "    def create_animation(self,name):\n",
    "\n",
    "        for t,w in enumerate(self.W):\n",
    "            self.create_frame(t,w,name)\n",
    "\n",
    "        frames = []\n",
    "        for t in range(len(self.W)):\n",
    "            image = imageio.v2.imread(f'./{name}/img_{t}.png')\n",
    "            frames.append(image)\n",
    "\n",
    "        imageio.mimsave(f'./{name}.gif', frames, fps = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65c609c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history of weight vectors\n",
    "W = p.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f741168",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize(X_train_normalized,y_train,W).create_animation('train_lin_sep_')\n",
    "Visualize(X_test_normalized,y_test,W).create_animation('test_lin_sep_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train_lin_sep_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_lin_sep_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1f7d2e1",
   "metadata": {},
   "source": [
    "### 2. Data with Noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba956aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading the data\n",
    "with open('Xlinnoise_sep.npy','rb') as f:\n",
    "    X = np.load(f)\n",
    "with open('ylinnoise_sep.npy','rb') as f:\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "282d1aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to set lables as binary 0 1\n",
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "997841fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=158)\n",
    "\n",
    "#normalize training set\n",
    "X_train_normalized = normalize_features(X_train)\n",
    "\n",
    "#normalizing test set\n",
    "X_test_normalized = normalize_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e2be954",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the model\n",
    "p = Perceptron(X_train_normalized,y_train,iterations=50)\n",
    "#fitting the perceptron\n",
    "p.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2f0e956",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get predictions on train and test set\n",
    "preds_train = p.predict(X_train_normalized)\n",
    "preds_test = p.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a33b2d36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on Train Set is 0.7866666666666666\n",
      "The accuracy on Test Set is 0.92\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy on Train Set is',accuracy_score(y_train,preds_train>0.5))\n",
    "print('The accuracy on Test Set is',accuracy_score(y_test,preds_test>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc25984a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history of weight vectors\n",
    "W = p.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "629e86cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize(X_train_normalized,y_train,W).create_animation('train_linnoise_sep_')\n",
    "Visualize(X_test_normalized,y_test,W).create_animation('test_linnoise_sep_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train_linnoise_sep_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_linnoise_sep_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac869076",
   "metadata": {},
   "source": [
    "### 3. Circle Data (Non-Linear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6438b943",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loading the data\n",
    "with open('circles_x.npy','rb') as f:\n",
    "    X = np.load(f)\n",
    "with open('circles_y.npy','rb') as f:\n",
    "    y = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "50c536f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to set lables as binary 0 1\n",
    "y[y == -1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa6f4a9",
   "metadata": {},
   "source": [
    "Since the data is not linearly separable, we will add polynomial features into the data. This means instead of giving the perceptron x1 and x2 as input we will now give it  $ x_1^2, x_2^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1f122155",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(np.square(X), y, test_size=0.25, random_state=42)\n",
    "\n",
    "#normalize training set\n",
    "X_train_normalized = normalize_features(X_train)\n",
    "\n",
    "#normalizing test set\n",
    "X_test_normalized = normalize_features(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f74def21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#making the model\n",
    "p = Perceptron(X_train_normalized,y_train,iterations=100)\n",
    "#fitting the perceptron\n",
    "p.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "239d41a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to get predictions on train and test set\n",
    "preds_train = p.predict(X_train_normalized)\n",
    "preds_test = p.predict(X_test_normalized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "355ae277",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy on Train Set is 1.0\n",
      "The accuracy on Test Set is 1.0\n"
     ]
    }
   ],
   "source": [
    "print('The accuracy on Train Set is',accuracy_score(y_train,preds_train>0.5))\n",
    "print('The accuracy on Test Set is',accuracy_score(y_test,preds_test>0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b41dfb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#history of weight vectors\n",
    "W = p.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e4f7ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Visualize(X_train_normalized,y_train,W).create_animation('train_circle_')\n",
    "Visualize(X_test_normalized,y_test,W).create_animation('test_circle_')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"train_circle_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"test_circle_.gif\" width=\"350\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a38fc5",
   "metadata": {},
   "source": [
    "### 2. Feed-Forward Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "4c2f3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "##Note: The derivatives of the activation functions are formulated in a way to support the presented implementation of NN\n",
    "## In reality the derivative of tanh is 1 - tanh(x)^2 and not 1 - x^2\n",
    "\n",
    "def ReLU(x,y=None):\n",
    "    return np.maximum(0,x), np.where(x > 0, 1, 0)\n",
    "\n",
    "def tanh(x,y=None):\n",
    "    v = np.divide((np.exp(x) - np.exp(-x)),(np.exp(x) + np.exp(-x)))\n",
    "    d = 1 - np.square(x) #original derivative of function is 1 - np.square(v)\n",
    "    return v,d\n",
    "\n",
    "def softmax(x, y=None):\n",
    "    e = np.exp(x)\n",
    "    s = np.sum(e,axis=1,keepdims=True)\n",
    "    p = np.divide(e,s)\n",
    "    if y is not None:\n",
    "        m = x.shape[0]\n",
    "        d = x.copy()\n",
    "        d[np.arange(m),y] -= 1\n",
    "        return p, d/m \n",
    "    else:\n",
    "        return p, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN:\n",
    "    def __init__ (self, X, y, num_neurons, activations, num_classes, dropout):\n",
    "        self.X = X #features\n",
    "        self.y = y #targets\n",
    "        self.activations = activations #activation functions as list\n",
    "        self.num_neurons = num_neurons #number of neurons in each layer as list\n",
    "        self.m, self.n = X.shape #number of obs, num of features\n",
    "        self.num_classes = num_classes #number of classes\n",
    "        self.dropout = dropout\n",
    "    \n",
    "    #to initialize the weights of the vectors, Kaiming Initialization is used to avoid exploding gradient problem\n",
    "    def initialize_random_weights(self):\n",
    "        layers_weights = [] #list to store the weights of each layer\n",
    "        input_dim = self.n #input dimension\n",
    "        \n",
    "        #iterate over each layer of neural network and initialize the random weights for its neurons\n",
    "        for l,n in enumerate(self.num_neurons): #for all layers\n",
    "            \n",
    "            #using kaiming initializations to avoid very small and very large gradient values\n",
    "            w = np.random.randn(input_dim, n) * np.sqrt(2.0 / input_dim)\n",
    "            b = np.ones((1, n)) #bias term\n",
    "            \n",
    "            layers_weights.append((w,b)) #append weights and biases of the layer as tuple in a list\n",
    "            input_dim = n #the rows of next layer is going to be equal to columns of the previous layer\n",
    "        \n",
    "        #weights and biases of the last layer using kaiming initialization scheme\n",
    "        w = np.random.randn(input_dim,self.num_classes)*np.sqrt(2.0 / input_dim)\n",
    "        b = np.ones(shape=(1,self.num_classes))\n",
    "        \n",
    "        #append layer weights of last layer\n",
    "        layers_weights.append((w,b))\n",
    "        \n",
    "        return layers_weights #returns the weights and biases of all layers\n",
    "     \n",
    "    #to calculate the cross entropy loss based on the current weights of the network\n",
    "    def calculate_loss(self, inputs, layers_weights, y):\n",
    "        m = inputs.shape[0] #number of sample in current batch\n",
    "        probs = self.forward_pass(inputs, layers_weights)[0] #forward pass of the network\n",
    "        y_pred = probs[np.arange(m),y] #obtain the probability of the correct class\n",
    "        loss = np.sum(-np.log(y_pred)/m) #summing the CE-Loss for each observation and normalizing it by number of samples\n",
    "        return loss #return the cross entropy loss\n",
    "    \n",
    "    #forward pass of the Neural Network given inputs and layer weights\n",
    "    def forward_pass(self, inputs, layers_weights):\n",
    "        outputs_cached = [inputs] #cache the activations of each layer to be used later in backprop\n",
    "        \n",
    "        for w, a in zip(layers_weights,self.activations): #iterating over one layer at a time\n",
    "            out = inputs @ w[0] + w[1] #taking the weighted sum of the inputs\n",
    "            out = a(out)[0] #applying the activation function on the weighted sum of inputs\n",
    "            outputs_cached.append(out) #caching the outputs after activations(to be used in backprop)\n",
    "            inputs = out #setting the output of previous layer as input to next layer\n",
    "        return out,outputs_cached #returning the output of final layer and cached activations of all hidden layers\n",
    "    \n",
    "    #calculating gradient of the network (alternatively called as back propagation)\n",
    "    def calculate_gradients(self, inputs, layers_weights, y):\n",
    "        #to store the gradients of each layer\n",
    "        grads = []\n",
    "        #getting the output and cached activations of the network based on current weights\n",
    "        _, cached_output = self.forward_pass(inputs,layers_weights)\n",
    "        #number of samples in current batch\n",
    "        m = inputs.shape[0]\n",
    "        \n",
    "        dz = True #set the initial accumulated to any arbitrary value\n",
    "        a = cached_output.pop(-1) #to get the activations of last layer\n",
    "        \n",
    "        #iterating over all layers and their activations from last to first this is why the procedure is called backprop\n",
    "        for lw,act in zip(reversed(layers_weights),reversed(self.activations)): \n",
    "            dz = np.multiply(dz,act(a,y)[1]) \n",
    "            a = cached_output.pop(-1) #to get the activations of previous layer\n",
    "            \n",
    "            dw = np.dot(a.T,dz) #the derivative of weights of current layer\n",
    "            db = np.sum(dz, axis = 0, keepdims=True) #the derivative of biases of current layer\n",
    "            \n",
    "            grads.append((dw,db)) #to store the gradients of weights and biases as a tuple\n",
    "            \n",
    "            dz = dz @ lw[0].T  #to update the accumulated gradient\n",
    "        \n",
    "        return grads[::-1] #since the gradients are calculated from last to first, reverse the gradient list\n",
    "    \n",
    "    #early stopping to avoid overfitting on training data\n",
    "    def early_stopping(self,current_loss,weights,min_delta=1e-9,patience=5):\n",
    "        #checks if the current loss is lower than the lowest loss, if not increment the counter\n",
    "        if self.best_loss is not None and (current_loss - min_delta) > self.best_loss:\n",
    "            self.wait += 1 #If the metric is not improving, increment the wait counter\n",
    "            if self.wait >= patience: #if the loss stops decreasing for some time, stop the training\n",
    "                self.stop_training = True #to stop training\n",
    "        #if the loss is lower than the lowest loss\n",
    "        else:\n",
    "            self.wait = 0 #resets the counter\n",
    "            self.best_loss = current_loss #sets current loss as lowest loss\n",
    "            self.weights = weights #best weights soo far\n",
    "    \n",
    "    #dropout layer to drop random neurons from each layer\n",
    "    def dropout_weights(self,lw,dp_list):\n",
    "        dropped_weights = [] #to store weights after turning off some neurons in each layer\n",
    "        for ws,p in zip(lw[:-1],dp_list): #iterate over all layers till last layer\n",
    "            #to make copy of data to avoid overwritting\n",
    "            w = ws[0].copy()\n",
    "            b = ws[1].copy()\n",
    "            \n",
    "            #number of neurons in each layer\n",
    "            n_neurons = w.shape[1]\n",
    "            \n",
    "            #randomly choose some neurons based on the proportion of dropout probability\n",
    "            r_n = np.random.choice(np.arange(n_neurons),int(n_neurons*p),replace=False)\n",
    "            \n",
    "            #set weights to zero\n",
    "            w[:,r_n] = 0\n",
    "            b[:,r_n] = 0\n",
    "            \n",
    "            dropped_weights.append((w*1/(1-p),b*1/(1-p))) #multiply the layers with inverse of keep probability (normalization)\n",
    "        dropped_weights.append(lw[-1]) #add last layer too\n",
    "        \n",
    "        return dropped_weights #return the weights after dropping some neurons\n",
    "    \n",
    "    #main function to fit the neural network to given data\n",
    "    def fit(self, num_epochs, lr, validation_data=None, batch_size=32):\n",
    "        if batch_size == None:\n",
    "            batch_size = self.m//num_epochs #batch size, to be feed into the network for each iteration\n",
    "        batch_ids = np.arange(0,self.m,batch_size) #start and end indexes of the data based on the size of batch\n",
    "        lw = self.initialize_random_weights() #initialize the weights of the network\n",
    "        curr_loss = self.calculate_loss(self.X,lw,self.y) #calculate the loss before the update\n",
    "        self.wait = 0 #the wait counter to be used in early stopping\n",
    "        self.best_loss = None #best loss to stop training\n",
    "        self.stop_training = False #to stop training\n",
    "        \n",
    "        #iterating over number of epochs\n",
    "        for epoch in range(num_epochs):\n",
    "            #shuffle the data each time before splitting into the batches and feeding in the network \n",
    "            x_full , y_full = shuffle(self.X,self.y)\n",
    "            #prints the current loss and epoch number\n",
    "            print('Currently at',epoch,'Epoch, Current Training Loss is',np.mean(curr_loss))\n",
    "            \n",
    "            #if validation data is provided, use early stopping to stop training\n",
    "            if validation_data:\n",
    "                x_valid,y_valid = validation_data #target and features of validation data\n",
    "                test_loss = self.calculate_loss(x_valid,lw,y_valid) #calculate the validation loss\n",
    "                print('Testing loss is',test_loss,'\\n') #printing the loss\n",
    "                self.early_stopping(test_loss,lw) #for early stopping\n",
    "            \n",
    "            #if the loss stops decreasing, stop the training procedure\n",
    "            if self.stop_training:\n",
    "                print('Early Stopped..')\n",
    "                return None\n",
    "            \n",
    "            #list to store the losses of each iteration\n",
    "            curr_loss = []\n",
    "            \n",
    "            #iterating over each mini batch (SGD)\n",
    "            for i in range(len(batch_ids[:-1])):\n",
    "                si = batch_ids[i] #start index of the current batch\n",
    "                ei = batch_ids[i+1] #end index of the current batch\n",
    "                x = x_full[si:ei] #batch features\n",
    "                y = y_full[si:ei] #batch targets\n",
    "                if self.dropout:\n",
    "                    lwa = self.dropout_weights(lw,self.dropout)\n",
    "                else:\n",
    "                    lwa = lw\n",
    "                grads = self.calculate_gradients(x, lwa, y) #gradients of the weights and biases of the layers\n",
    "                lwu = [] #store the weights after SGD\n",
    "                \n",
    "                #iterate over each layer\n",
    "                for w,g in zip(lw,grads):\n",
    "                    #to update weights and biases of the layers\n",
    "                    wu = w[0] - lr*g[0]\n",
    "                    bu = w[1] - lr*g[1]\n",
    "                    #store the updated weights and biases\n",
    "                    lwu.append((wu,bu))\n",
    "                \n",
    "                lw = lwu #set the new weights as the current weights\n",
    "                curr_loss.append(self.calculate_loss(self.X,lwu,self.y)) #append the loss of current network\n",
    "                \n",
    "        self.weights = lwu #once training is done, set the weights in class variable\n",
    "        \n",
    "    #prediction function to output the class of current inputs    \n",
    "    def predict(self,x): \n",
    "        #to compute probabilites of given inputs\n",
    "        probs = self.forward_pass(x,self.weights)[0]\n",
    "        return np.argmax(probs,axis=1) #output the class label of the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "3ed844f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to load the MNIST Data by SKlearn\n",
    "digits = load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4c69568d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#to separate in the features and labels\n",
    "X = digits['data']\n",
    "y = digits['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "64e4fdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scalar for scaling the features aka pixels\n",
    "scalar = StandardScaler()\n",
    "X = scalar.fit_transform(X) \n",
    "#train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#train valid split\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "daea1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model - with dropout\n",
    "model = NN(X = X_train,y = y_train, num_neurons=[50,20], activations=[tanh,ReLU,softmax],num_classes=10,dropout=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "739a147b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at 0 Epoch, Current Training Loss is 4.378131214017997\n",
      "Testing loss is 4.482959084163465 \n",
      "\n",
      "Currently at 1 Epoch, Current Training Loss is 0.7739740011718188\n",
      "Testing loss is 0.34949843285594573 \n",
      "\n",
      "Currently at 2 Epoch, Current Training Loss is 0.19879634258885828\n",
      "Testing loss is 0.2239722330052484 \n",
      "\n",
      "Currently at 3 Epoch, Current Training Loss is 0.10341943068953463\n",
      "Testing loss is 0.16568576307182253 \n",
      "\n",
      "Currently at 4 Epoch, Current Training Loss is 0.06534101245096022\n",
      "Testing loss is 0.14696100129106462 \n",
      "\n",
      "Currently at 5 Epoch, Current Training Loss is 0.045175616319192514\n",
      "Testing loss is 0.1309491249776022 \n",
      "\n",
      "Currently at 6 Epoch, Current Training Loss is 0.033650089352488546\n",
      "Testing loss is 0.12531940964932023 \n",
      "\n",
      "Currently at 7 Epoch, Current Training Loss is 0.025021539416859195\n",
      "Testing loss is 0.12145273812169163 \n",
      "\n",
      "Currently at 8 Epoch, Current Training Loss is 0.020029986237009544\n",
      "Testing loss is 0.11170412508640748 \n",
      "\n",
      "Currently at 9 Epoch, Current Training Loss is 0.015995466083621856\n",
      "Testing loss is 0.1105789656448527 \n",
      "\n",
      "Currently at 10 Epoch, Current Training Loss is 0.013199237493357063\n",
      "Testing loss is 0.10436392209394954 \n",
      "\n",
      "Currently at 11 Epoch, Current Training Loss is 0.011359015435959664\n",
      "Testing loss is 0.10071831388799683 \n",
      "\n",
      "Currently at 12 Epoch, Current Training Loss is 0.009887153657526977\n",
      "Testing loss is 0.09930116190554675 \n",
      "\n",
      "Currently at 13 Epoch, Current Training Loss is 0.008519598821720547\n",
      "Testing loss is 0.10180117256782133 \n",
      "\n",
      "Currently at 14 Epoch, Current Training Loss is 0.00761702675661248\n",
      "Testing loss is 0.09890865490742852 \n",
      "\n",
      "Currently at 15 Epoch, Current Training Loss is 0.006824512850987674\n",
      "Testing loss is 0.09689591135881861 \n",
      "\n",
      "Currently at 16 Epoch, Current Training Loss is 0.006231420190990093\n",
      "Testing loss is 0.09443800731307633 \n",
      "\n",
      "Currently at 17 Epoch, Current Training Loss is 0.0056759192535180665\n",
      "Testing loss is 0.09410663710016781 \n",
      "\n",
      "Currently at 18 Epoch, Current Training Loss is 0.005231349946667978\n",
      "Testing loss is 0.0933724381147503 \n",
      "\n",
      "Currently at 19 Epoch, Current Training Loss is 0.004814515393849694\n",
      "Testing loss is 0.09354554315117189 \n",
      "\n",
      "Currently at 20 Epoch, Current Training Loss is 0.004481102254775148\n",
      "Testing loss is 0.09341532328161636 \n",
      "\n",
      "Currently at 21 Epoch, Current Training Loss is 0.004180490690109459\n",
      "Testing loss is 0.09223696154284025 \n",
      "\n",
      "Currently at 22 Epoch, Current Training Loss is 0.003902066967319017\n",
      "Testing loss is 0.09214765282889353 \n",
      "\n",
      "Currently at 23 Epoch, Current Training Loss is 0.0036557226545771747\n",
      "Testing loss is 0.09065694810945597 \n",
      "\n",
      "Currently at 24 Epoch, Current Training Loss is 0.003453641620814583\n",
      "Testing loss is 0.09177628640199431 \n",
      "\n",
      "Currently at 25 Epoch, Current Training Loss is 0.003275876206591337\n",
      "Testing loss is 0.08886356588987351 \n",
      "\n",
      "Currently at 26 Epoch, Current Training Loss is 0.003091736172048104\n",
      "Testing loss is 0.0902038314527878 \n",
      "\n",
      "Currently at 27 Epoch, Current Training Loss is 0.0029408989161405982\n",
      "Testing loss is 0.08984271992358304 \n",
      "\n",
      "Currently at 28 Epoch, Current Training Loss is 0.0027954889400943752\n",
      "Testing loss is 0.08910811136159942 \n",
      "\n",
      "Currently at 29 Epoch, Current Training Loss is 0.0026686022784633842\n",
      "Testing loss is 0.08898826863602029 \n",
      "\n",
      "Currently at 30 Epoch, Current Training Loss is 0.002546160608306742\n",
      "Testing loss is 0.08874028540427766 \n",
      "\n",
      "Currently at 31 Epoch, Current Training Loss is 0.0024352607908709753\n",
      "Testing loss is 0.0884342047661005 \n",
      "\n",
      "Currently at 32 Epoch, Current Training Loss is 0.002340828745343452\n",
      "Testing loss is 0.08753419357257858 \n",
      "\n",
      "Currently at 33 Epoch, Current Training Loss is 0.0022421571670145322\n",
      "Testing loss is 0.08676265658816898 \n",
      "\n",
      "Currently at 34 Epoch, Current Training Loss is 0.002156633734968897\n",
      "Testing loss is 0.08669727744816391 \n",
      "\n",
      "Currently at 35 Epoch, Current Training Loss is 0.002078457119943205\n",
      "Testing loss is 0.08620412430450232 \n",
      "\n",
      "Currently at 36 Epoch, Current Training Loss is 0.0020021041154631328\n",
      "Testing loss is 0.085682491479487 \n",
      "\n",
      "Currently at 37 Epoch, Current Training Loss is 0.0019327326904878775\n",
      "Testing loss is 0.0862097401355686 \n",
      "\n",
      "Currently at 38 Epoch, Current Training Loss is 0.0018659710597185627\n",
      "Testing loss is 0.08616347894205173 \n",
      "\n",
      "Currently at 39 Epoch, Current Training Loss is 0.0018013727906884733\n",
      "Testing loss is 0.08574352203615179 \n",
      "\n",
      "Currently at 40 Epoch, Current Training Loss is 0.0017469877828609683\n",
      "Testing loss is 0.08613261334783741 \n",
      "\n",
      "Currently at 41 Epoch, Current Training Loss is 0.0016875829431995172\n",
      "Testing loss is 0.08564431555606988 \n",
      "\n",
      "Currently at 42 Epoch, Current Training Loss is 0.0016390116793196523\n",
      "Testing loss is 0.08525000116830342 \n",
      "\n",
      "Currently at 43 Epoch, Current Training Loss is 0.0015882408571748642\n",
      "Testing loss is 0.0850814615988983 \n",
      "\n",
      "Currently at 44 Epoch, Current Training Loss is 0.0015428568240258833\n",
      "Testing loss is 0.08462296245814302 \n",
      "\n",
      "Currently at 45 Epoch, Current Training Loss is 0.0014992920058020764\n",
      "Testing loss is 0.0842716250415155 \n",
      "\n",
      "Currently at 46 Epoch, Current Training Loss is 0.001455390574306311\n",
      "Testing loss is 0.08423920210933142 \n",
      "\n",
      "Currently at 47 Epoch, Current Training Loss is 0.0014192448727051513\n",
      "Testing loss is 0.08442018431031194 \n",
      "\n",
      "Currently at 48 Epoch, Current Training Loss is 0.0013787449959629032\n",
      "Testing loss is 0.08402708359482006 \n",
      "\n",
      "Currently at 49 Epoch, Current Training Loss is 0.0013452206975984784\n",
      "Testing loss is 0.0837214109114689 \n",
      "\n",
      "Currently at 50 Epoch, Current Training Loss is 0.0013101837813496837\n",
      "Testing loss is 0.08351342570679543 \n",
      "\n",
      "Currently at 51 Epoch, Current Training Loss is 0.001277856730180122\n",
      "Testing loss is 0.08354283745173538 \n",
      "\n",
      "Currently at 52 Epoch, Current Training Loss is 0.0012463239237069917\n",
      "Testing loss is 0.08332471913817961 \n",
      "\n",
      "Currently at 53 Epoch, Current Training Loss is 0.0012193255226322012\n",
      "Testing loss is 0.08361193752708326 \n",
      "\n",
      "Currently at 54 Epoch, Current Training Loss is 0.0011906013865028591\n",
      "Testing loss is 0.08325516845741167 \n",
      "\n",
      "Currently at 55 Epoch, Current Training Loss is 0.001162396987902281\n",
      "Testing loss is 0.08315620071890131 \n",
      "\n",
      "Currently at 56 Epoch, Current Training Loss is 0.0011359871800083747\n",
      "Testing loss is 0.08320651247621831 \n",
      "\n",
      "Currently at 57 Epoch, Current Training Loss is 0.0011111266507558785\n",
      "Testing loss is 0.08305485717788845 \n",
      "\n",
      "Currently at 58 Epoch, Current Training Loss is 0.0010866799938249838\n",
      "Testing loss is 0.08284906253709402 \n",
      "\n",
      "Currently at 59 Epoch, Current Training Loss is 0.0010629917131439535\n",
      "Testing loss is 0.08266193946086735 \n",
      "\n",
      "Currently at 60 Epoch, Current Training Loss is 0.0010414724963521602\n",
      "Testing loss is 0.08250775239666686 \n",
      "\n",
      "Currently at 61 Epoch, Current Training Loss is 0.001020725980039806\n",
      "Testing loss is 0.08253260146001197 \n",
      "\n",
      "Currently at 62 Epoch, Current Training Loss is 0.0010000103363623364\n",
      "Testing loss is 0.08229195796248437 \n",
      "\n",
      "Currently at 63 Epoch, Current Training Loss is 0.0009798935744546587\n",
      "Testing loss is 0.08229415631113504 \n",
      "\n",
      "Currently at 64 Epoch, Current Training Loss is 0.0009605982752707153\n",
      "Testing loss is 0.08212492475905724 \n",
      "\n",
      "Currently at 65 Epoch, Current Training Loss is 0.0009417976935610685\n",
      "Testing loss is 0.08201646372665986 \n",
      "\n",
      "Currently at 66 Epoch, Current Training Loss is 0.0009241955480685802\n",
      "Testing loss is 0.08180496418185676 \n",
      "\n",
      "Currently at 67 Epoch, Current Training Loss is 0.000906640602558725\n",
      "Testing loss is 0.08180734655172536 \n",
      "\n",
      "Currently at 68 Epoch, Current Training Loss is 0.0008909658870427627\n",
      "Testing loss is 0.0818172559808853 \n",
      "\n",
      "Currently at 69 Epoch, Current Training Loss is 0.0008746641349851085\n",
      "Testing loss is 0.08163703801108137 \n",
      "\n",
      "Currently at 70 Epoch, Current Training Loss is 0.0008594203093797001\n",
      "Testing loss is 0.08146797670452804 \n",
      "\n",
      "Currently at 71 Epoch, Current Training Loss is 0.0008448392299147921\n",
      "Testing loss is 0.08131888393293688 \n",
      "\n",
      "Currently at 72 Epoch, Current Training Loss is 0.0008300358056047647\n",
      "Testing loss is 0.08136969828189594 \n",
      "\n",
      "Currently at 73 Epoch, Current Training Loss is 0.0008155721781990827\n",
      "Testing loss is 0.0811537981341911 \n",
      "\n",
      "Currently at 74 Epoch, Current Training Loss is 0.0008024845933244579\n",
      "Testing loss is 0.08123522872775896 \n",
      "\n",
      "Currently at 75 Epoch, Current Training Loss is 0.0007892164138962119\n",
      "Testing loss is 0.08099070030094246 \n",
      "\n",
      "Currently at 76 Epoch, Current Training Loss is 0.0007762830719698884\n",
      "Testing loss is 0.08093993099401457 \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at 77 Epoch, Current Training Loss is 0.0007639684895194713\n",
      "Testing loss is 0.08079913336654843 \n",
      "\n",
      "Currently at 78 Epoch, Current Training Loss is 0.0007516890498718571\n",
      "Testing loss is 0.08071820159640071 \n",
      "\n",
      "Currently at 79 Epoch, Current Training Loss is 0.0007401022611635017\n",
      "Testing loss is 0.08069486678382612 \n",
      "\n",
      "Currently at 80 Epoch, Current Training Loss is 0.0007283894243981776\n",
      "Testing loss is 0.08042463090765267 \n",
      "\n",
      "Currently at 81 Epoch, Current Training Loss is 0.0007175741396551801\n",
      "Testing loss is 0.08069559533736678 \n",
      "\n",
      "Currently at 82 Epoch, Current Training Loss is 0.0007073914493938999\n",
      "Testing loss is 0.0804866770077827 \n",
      "\n",
      "Currently at 83 Epoch, Current Training Loss is 0.0006969090091509229\n",
      "Testing loss is 0.08020806383054524 \n",
      "\n",
      "Currently at 84 Epoch, Current Training Loss is 0.0006865347345547132\n",
      "Testing loss is 0.0802156638569886 \n",
      "\n",
      "Currently at 85 Epoch, Current Training Loss is 0.0006764887054243548\n",
      "Testing loss is 0.08022232033925702 \n",
      "\n",
      "Currently at 86 Epoch, Current Training Loss is 0.0006663639710622531\n",
      "Testing loss is 0.08015140743097365 \n",
      "\n",
      "Currently at 87 Epoch, Current Training Loss is 0.0006575729738091971\n",
      "Testing loss is 0.07995278678117017 \n",
      "\n",
      "Currently at 88 Epoch, Current Training Loss is 0.0006483947474568435\n",
      "Testing loss is 0.07988346043356555 \n",
      "\n",
      "Currently at 89 Epoch, Current Training Loss is 0.0006391267533251861\n",
      "Testing loss is 0.07981825628435174 \n",
      "\n",
      "Currently at 90 Epoch, Current Training Loss is 0.0006307334114591396\n",
      "Testing loss is 0.07978977407670129 \n",
      "\n",
      "Currently at 91 Epoch, Current Training Loss is 0.0006221535318161405\n",
      "Testing loss is 0.07964571801092388 \n",
      "\n",
      "Currently at 92 Epoch, Current Training Loss is 0.0006140596226281006\n",
      "Testing loss is 0.07974074208642393 \n",
      "\n",
      "Currently at 93 Epoch, Current Training Loss is 0.000605658063997297\n",
      "Testing loss is 0.07969746991082396 \n",
      "\n",
      "Currently at 94 Epoch, Current Training Loss is 0.0005977020933107738\n",
      "Testing loss is 0.07945851695556054 \n",
      "\n",
      "Currently at 95 Epoch, Current Training Loss is 0.0005903974626048586\n",
      "Testing loss is 0.07934520287339869 \n",
      "\n",
      "Currently at 96 Epoch, Current Training Loss is 0.0005826543181650611\n",
      "Testing loss is 0.07939400760026999 \n",
      "\n",
      "Currently at 97 Epoch, Current Training Loss is 0.0005755008831058987\n",
      "Testing loss is 0.07936230860562282 \n",
      "\n",
      "Currently at 98 Epoch, Current Training Loss is 0.0005680428682746718\n",
      "Testing loss is 0.0793488658375075 \n",
      "\n",
      "Currently at 99 Epoch, Current Training Loss is 0.0005612257349945575\n",
      "Testing loss is 0.0792696151544409 \n",
      "\n",
      "Currently at 100 Epoch, Current Training Loss is 0.0005544592074509568\n",
      "Testing loss is 0.07914549441399707 \n",
      "\n",
      "Currently at 101 Epoch, Current Training Loss is 0.0005477367076817136\n",
      "Testing loss is 0.07907094785591715 \n",
      "\n",
      "Currently at 102 Epoch, Current Training Loss is 0.0005412776276378855\n",
      "Testing loss is 0.07886551567913544 \n",
      "\n",
      "Currently at 103 Epoch, Current Training Loss is 0.0005350161108814656\n",
      "Testing loss is 0.07882761132530977 \n",
      "\n",
      "Currently at 104 Epoch, Current Training Loss is 0.00052828625987065\n",
      "Testing loss is 0.07881429097465886 \n",
      "\n",
      "Currently at 105 Epoch, Current Training Loss is 0.0005223858039394305\n",
      "Testing loss is 0.07878076999276006 \n",
      "\n",
      "Currently at 106 Epoch, Current Training Loss is 0.000516504514332897\n",
      "Testing loss is 0.0787171612268947 \n",
      "\n",
      "Currently at 107 Epoch, Current Training Loss is 0.00051058316137951\n",
      "Testing loss is 0.07865961442658144 \n",
      "\n",
      "Currently at 108 Epoch, Current Training Loss is 0.0005047533460076362\n",
      "Testing loss is 0.07857516855841594 \n",
      "\n",
      "Currently at 109 Epoch, Current Training Loss is 0.0004992522903956386\n",
      "Testing loss is 0.07852593468848236 \n",
      "\n",
      "Currently at 110 Epoch, Current Training Loss is 0.0004939115888407486\n",
      "Testing loss is 0.07839697563874284 \n",
      "\n",
      "Currently at 111 Epoch, Current Training Loss is 0.0004882586212861751\n",
      "Testing loss is 0.07842519237477685 \n",
      "\n",
      "Currently at 112 Epoch, Current Training Loss is 0.00048317663508853193\n",
      "Testing loss is 0.07835238273543535 \n",
      "\n",
      "Currently at 113 Epoch, Current Training Loss is 0.0004780815353639486\n",
      "Testing loss is 0.07842367386380172 \n",
      "\n",
      "Currently at 114 Epoch, Current Training Loss is 0.00047298700814655344\n",
      "Testing loss is 0.07839848210332723 \n",
      "\n",
      "Currently at 115 Epoch, Current Training Loss is 0.00046810259701787776\n",
      "Testing loss is 0.07848943345290962 \n",
      "\n",
      "Currently at 116 Epoch, Current Training Loss is 0.00046327225023402115\n",
      "Testing loss is 0.07854570111517847 \n",
      "\n",
      "Currently at 117 Epoch, Current Training Loss is 0.0004585902364872881\n",
      "Testing loss is 0.07846242433609686 \n",
      "\n",
      "Early Stopped..\n"
     ]
    }
   ],
   "source": [
    "model.fit(num_epochs = 120, lr = 0.2, validation_data=(X_valid,y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "6e20cf0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the Neural Network on test set is 0.9638888888888889\n"
     ]
    }
   ],
   "source": [
    "print('The Accuracy of the Neural Network on test set is',accuracy_score(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. **Bonus: Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fitting the model - with dropout\n",
    "model = NN(X = X_train,y = y_train, num_neurons=[50,20], activations=[tanh,ReLU,softmax],num_classes=10,dropout=[0.2,0.3,None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "428f2844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently at 0 Epoch, Current Training Loss is 4.082958340459529\n",
      "Currently at 1 Epoch, Current Training Loss is 1.1136455540057433\n",
      "Currently at 2 Epoch, Current Training Loss is 0.3643219508175049\n",
      "Currently at 3 Epoch, Current Training Loss is 0.19032758683142878\n",
      "Currently at 4 Epoch, Current Training Loss is 0.14508525447646195\n",
      "Currently at 5 Epoch, Current Training Loss is 0.10340994078174832\n",
      "Currently at 6 Epoch, Current Training Loss is 0.08025315975532595\n",
      "Currently at 7 Epoch, Current Training Loss is 0.0680460087321394\n",
      "Currently at 8 Epoch, Current Training Loss is 0.05908268640847355\n",
      "Currently at 9 Epoch, Current Training Loss is 0.04655971129008196\n",
      "Currently at 10 Epoch, Current Training Loss is 0.04456253471798619\n",
      "Currently at 11 Epoch, Current Training Loss is 0.036498629964305127\n",
      "Currently at 12 Epoch, Current Training Loss is 0.033735897690187235\n",
      "Currently at 13 Epoch, Current Training Loss is 0.03019930492167891\n",
      "Currently at 14 Epoch, Current Training Loss is 0.02658314228155546\n",
      "Currently at 15 Epoch, Current Training Loss is 0.027296935452735083\n",
      "Currently at 16 Epoch, Current Training Loss is 0.02354124174723468\n",
      "Currently at 17 Epoch, Current Training Loss is 0.02100367086488384\n",
      "Currently at 18 Epoch, Current Training Loss is 0.0189170119918106\n",
      "Currently at 19 Epoch, Current Training Loss is 0.018847386274847305\n"
     ]
    }
   ],
   "source": [
    "#here we will not use validation data\n",
    "model.fit(num_epochs = 20, lr = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a80ada11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy of the Neural Network on test set with Dropout is 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "print('The Accuracy of the Neural Network on test set with Dropout is',accuracy_score(y_test,model.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bb7076",
   "metadata": {},
   "source": [
    "## 3. MLP using Sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87ba13c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d49f45e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_mlp = MLPClassifier(max_iter=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede7b682",
   "metadata": {},
   "source": [
    "We will test with random hidden layer size, random activation functions applied to each layer, the optimizer and Learning Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7835d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Grid\n",
    "grid = {\n",
    "    'hidden_layer_sizes': [(50,30,20),(50,25,10),(25,10)],\n",
    "    'activation': ['tanh', 'relu'],\n",
    "    'solver': ['sgd'],\n",
    "    'alpha': [0.001, 0.001, 0.01, 0.1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ad5455b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomizedSearchCV(cv=5, estimator=MLPClassifier(max_iter=1000), n_jobs=-1,\n",
       "                   param_distributions={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                        &#x27;alpha&#x27;: [0.001, 0.001, 0.01, 0.1],\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        &#x27;solver&#x27;: [&#x27;sgd&#x27;]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomizedSearchCV</label><div class=\"sk-toggleable__content\"><pre>RandomizedSearchCV(cv=5, estimator=MLPClassifier(max_iter=1000), n_jobs=-1,\n",
       "                   param_distributions={&#x27;activation&#x27;: [&#x27;tanh&#x27;, &#x27;relu&#x27;],\n",
       "                                        &#x27;alpha&#x27;: [0.001, 0.001, 0.01, 0.1],\n",
       "                                        &#x27;hidden_layer_sizes&#x27;: [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        &#x27;solver&#x27;: [&#x27;sgd&#x27;]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomizedSearchCV(cv=5, estimator=MLPClassifier(max_iter=1000), n_jobs=-1,\n",
       "                   param_distributions={'activation': ['tanh', 'relu'],\n",
       "                                        'alpha': [0.001, 0.001, 0.01, 0.1],\n",
       "                                        'hidden_layer_sizes': [(50, 30, 20),\n",
       "                                                               (50, 25, 10),\n",
       "                                                               (25, 10)],\n",
       "                                        'solver': ['sgd']})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = RandomizedSearchCV(sklearn_mlp, grid, n_jobs=-1, cv=5)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "a9a38b8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'solver': 'sgd',\n",
       " 'hidden_layer_sizes': (50, 30, 20),\n",
       " 'alpha': 0.001,\n",
       " 'activation': 'tanh'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037cc249",
   "metadata": {},
   "source": [
    "**Result:**\n",
    "The Neural Network with 3 layers with 50,30, and 20 neurons in each with activation function tanh, trained with SGD with Learning Rate 0.001, gave the best accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1531a254",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Accuracy on Test Data is 0.9638888888888889\n"
     ]
    }
   ],
   "source": [
    "print('The Accuracy on Test Data is',accuracy_score(y_test,clf.predict(X_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VoYX8BsKaVw-",
    "outputId": "a48473e7-7603-4469-c9f0-b81b10630e9a"
   },
   "source": [
    "## 5 **Bonus: Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "ou1cLNbNa8Ou"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "    from torch.optim import Adam\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first make a custom dataset class which will be used by the Pytorch Dataloader to load the data in batches and give it to the CNN. Here we will input it the Pandas Dataframe which contains two columns, First column is names of the images and second column contains their corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "Ue5hG9VKctHv"
   },
   "outputs": [],
   "source": [
    "class DrivingData(Dataset): #inherit from pytorch Dataset class\n",
    "    def __init__ (self, data_dir, data):\n",
    "        super(DrivingData,self).__init__() #initialize the parent class\n",
    "        self.data_dir = data_dir #directory where the images and labels are present\n",
    "        self.data = data #dataframe which contains two columns, name of the file and its label\n",
    "\n",
    "    def __len__ (self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    #to give freedom to torch to call as many sample as it wants depaneding on the batch size\n",
    "    def __getitem__(self, index):\n",
    "        img_name = self.data.iloc[index,0] #name of the particular image\n",
    "        img_path = os.path.join(self.data_dir, img_name) #complete path of the particular image\n",
    "        \n",
    "        #the input is rescaled to 66,200 to match the dimensions as presented in the paper\n",
    "        img = cv2.resize(cv2.cvtColor(cv2.imread(img_path),cv2.COLOR_BGR2RGB),(66,200)).reshape(3,66,200)\n",
    "        label = self.data.iloc[index,-1] #to take the label\n",
    "        return img,label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The architecture of the network is:\n",
    "<img src=\"architecture.png\" width=\"250\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "y924zzlkczwO"
   },
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__ (self, in_channels=3, out_channels=1):\n",
    "        super(NN,self).__init__()\n",
    "        self.in_channels = in_channels #input channels which is 3\n",
    "        self.norm = nn.LayerNorm([in_channels,66,200], elementwise_affine=False) #normalization layer: takes channel wise norm\n",
    "        self.c1 = nn.Conv2d(in_channels=in_channels, out_channels=24, kernel_size=5, stride=2) #first conv layer\n",
    "        self.c2 = nn.Conv2d(in_channels=24, out_channels=36, kernel_size=5, stride=2) #second conv layer\n",
    "        self.c3 = nn.Conv2d(in_channels=36, out_channels=48, kernel_size=5, stride=2) #third conv layer\n",
    "        self.c4 = nn.Conv2d(in_channels=48, out_channels=64, kernel_size=3, stride=1) #fourth conv layer\n",
    "        self.c5 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1) #fifth conv layer\n",
    "        self.fl = nn.Flatten() #flatten layer\n",
    "        self.fc1 = nn.Linear(in_features=1152, out_features=100) #first fc layer\n",
    "        self.fc2 = nn.Linear(in_features=100, out_features=50) #second fc layer\n",
    "        self.fc3 = nn.Linear(in_features=50, out_features=10) #third fc layer\n",
    "        self.fc = nn.Linear(in_features=10, out_features=out_channels) #final fc layer\n",
    "        self.act = nn.ELU() #activation function which is ELU in this case\n",
    "\n",
    "    def forward(self,x):\n",
    "        #forward pass of convolutional layers\n",
    "        conv_output = self.act(self.c5(self.act(self.c4(self.act(self.c3(self.act(self.c2(self.act(self.c1(x))))))))))\n",
    "        \n",
    "        #forward pass of fc layers till last layer (excluded)\n",
    "        fc_layers = self.act(self.fc3(self.act(self.fc2(self.act(self.fc1(self.fl(conv_output)))))))\n",
    "        \n",
    "        #forward pass of last fc layer\n",
    "        out = self.fc(fc_layers)\n",
    "        return out #return the prediction of steering angle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The early stopping helps us in avoiding overfitting. The below implementation of EarlyStopping is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1WELKJuquv6s"
   },
   "outputs": [],
   "source": [
    "##Implementation of Early Stopping Adopted from https://github.com/Bjarten/early-stopping-pytorch\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stops the training if validation loss doesn't improve after a given patience.\"\"\"\n",
    "    def __init__(self, patience=7, verbose=False, delta=0, path='checkpoint.pt', trace_func=print):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            patience (int): How long to wait after last time validation loss improved.\n",
    "                            Default: 7\n",
    "            verbose (bool): If True, prints a message for each validation loss improvement. \n",
    "                            Default: False\n",
    "            delta (float): Minimum change in the monitored quantity to qualify as an improvement.\n",
    "                            Default: 0\n",
    "            path (str): Path for the checkpoint to be saved to.\n",
    "                            Default: 'checkpoint.pt'\n",
    "            trace_func (function): trace print function.\n",
    "                            Default: print            \n",
    "        \"\"\"\n",
    "        self.patience = patience\n",
    "        self.verbose = verbose\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        self.val_loss_min = np.Inf\n",
    "        self.delta = delta\n",
    "        self.path = path\n",
    "        self.trace_func = trace_func\n",
    "    def __call__(self, val_loss, model):\n",
    "\n",
    "        score = -val_loss\n",
    "\n",
    "        if self.best_score is None:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "        elif score < self.best_score + self.delta:\n",
    "            self.counter += 1\n",
    "            self.trace_func(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_score = score\n",
    "            self.save_checkpoint(val_loss, model)\n",
    "            self.counter = 0\n",
    "\n",
    "    def save_checkpoint(self, val_loss, model):\n",
    "        '''Saves model when validation loss decrease.'''\n",
    "        if self.verbose:\n",
    "            self.trace_func(f'Validation loss decreased ({self.val_loss_min:.6f} --> {val_loss:.6f}).  Saving model ...')\n",
    "        torch.save(model.state_dict(), self.path)\n",
    "        self.val_loss_min = val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "UxMOL9ttc0wP"
   },
   "outputs": [],
   "source": [
    "## Hyperparams\n",
    "BATCH_SIZE = 128 #in each iteration batch of 128 images will be feed to CNN\n",
    "LR = 3e-4 #learning rate\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu') #this ensures to train on GPU if avialble\n",
    "NUM_EPOCHS = 500 #number of epochs, we can set it to any high number because we are using early stopping anyways\n",
    "PATIENCE = 7 #patience value for early stopping\n",
    "PATH = r\"/content/driving_dataset\" #path where the images and labels are stored\n",
    "\n",
    "## Defining Optimizer and Loss\n",
    "model = NN().double().to(DEVICE) #make the model, send it to GPU and set double precision to avoid errors\n",
    "optimizer = Adam(model.parameters(),lr=LR) #the adam optimizer is used to train the network\n",
    "criterion = nn.MSELoss() #MSE loss, when loss is applied, sqrt is taken to make it RMSE\n",
    "\n",
    "## Data Loaders\n",
    "data = pd.read_csv(os.path.join(PATH,'angles.txt'), header=None, sep=' ') #loading the label files which also has image names\n",
    "dt,dv = train_test_split(data.iloc[:-10000,:], test_size=0.2, random_state=42) #train validation split\n",
    "train_data = DrivingData(PATH, dt) #train data\n",
    "valid_data = DrivingData(PATH, dv) #validation data\n",
    "test_data = DrivingData(PATH, data.iloc[-10000:,:]) #testing data (last 10k images are used for testing)\n",
    "train_dataloader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True) #train data loader\n",
    "valid_dataloader = DataLoader(valid_data, batch_size=BATCH_SIZE) #validation data\n",
    "test_dataloader = DataLoader(test_data, batch_size=BATCH_SIZE) #testing data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WWx1FtZnd6Q",
    "outputId": "054d0333-3271-4d0c-c6ce-7b795bdb664d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0/500] train_loss: 31.18434 valid_loss: 32.25564\n",
      "\n",
      "Validation loss decreased (inf --> 32.255641).  Saving model ...\n",
      "[  1/500] train_loss: 30.94227 valid_loss: 32.25326\n",
      "\n",
      "Validation loss decreased (32.255641 --> 32.253258).  Saving model ...\n",
      "[  2/500] train_loss: 30.98432 valid_loss: 32.25113\n",
      "\n",
      "Validation loss decreased (32.253258 --> 32.251133).  Saving model ...\n",
      "[  3/500] train_loss: 30.96847 valid_loss: 32.25476\n",
      "\n",
      "EarlyStopping counter: 1 out of 7\n",
      "[  4/500] train_loss: 30.96962 valid_loss: 32.25139\n",
      "\n",
      "EarlyStopping counter: 2 out of 7\n",
      "[  5/500] train_loss: 31.00691 valid_loss: 32.25099\n",
      "\n",
      "Validation loss decreased (32.251133 --> 32.250990).  Saving model ...\n",
      "[  6/500] train_loss: 31.09519 valid_loss: 32.25281\n",
      "\n",
      "EarlyStopping counter: 1 out of 7\n",
      "[  7/500] train_loss: 31.02684 valid_loss: 32.25110\n",
      "\n",
      "EarlyStopping counter: 2 out of 7\n",
      "[  8/500] train_loss: 31.03365 valid_loss: 32.25144\n",
      "\n",
      "EarlyStopping counter: 3 out of 7\n",
      "[  9/500] train_loss: 31.01149 valid_loss: 32.25192\n",
      "\n",
      "EarlyStopping counter: 4 out of 7\n",
      "[ 10/500] train_loss: 31.01035 valid_loss: 32.25092\n",
      "\n",
      "Validation loss decreased (32.250990 --> 32.250924).  Saving model ...\n",
      "[ 11/500] train_loss: 30.87804 valid_loss: 32.25138\n",
      "\n",
      "EarlyStopping counter: 1 out of 7\n",
      "[ 12/500] train_loss: 31.03692 valid_loss: 32.25781\n",
      "\n",
      "EarlyStopping counter: 2 out of 7\n",
      "[ 13/500] train_loss: 31.01870 valid_loss: 32.25242\n",
      "\n",
      "EarlyStopping counter: 3 out of 7\n",
      "[ 14/500] train_loss: 30.99232 valid_loss: 32.25253\n",
      "\n",
      "EarlyStopping counter: 4 out of 7\n",
      "[ 15/500] train_loss: 30.99874 valid_loss: 32.25120\n",
      "\n",
      "EarlyStopping counter: 5 out of 7\n",
      "[ 16/500] train_loss: 30.95577 valid_loss: 32.25203\n",
      "\n",
      "EarlyStopping counter: 6 out of 7\n"
     ]
    }
   ],
   "source": [
    "#training loop is also taken from https://github.com/Bjarten/early-stopping-pytorch\n",
    "\n",
    "# to track the training loss as the model trains\n",
    "train_losses = []\n",
    "# to track the validation loss as the model trains\n",
    "valid_losses = []\n",
    "# to track the average training loss per epoch as the model trains\n",
    "avg_train_losses = []\n",
    "# to track the average validation loss per epoch as the model trains\n",
    "avg_valid_losses = [] \n",
    "\n",
    "# initialize the early_stopping object\n",
    "early_stopping = EarlyStopping(patience=PATIENCE, verbose=True)\n",
    "\n",
    "#iterate till max epochs\n",
    "for e in range(NUM_EPOCHS):\n",
    "    model.train() # prep model for training\n",
    "    for batch_idx, (data,target) in enumerate(train_dataloader): #iterating over each batch\n",
    "        data = data.to(DEVICE).double() #send data to GPU if available and set the data in double precision\n",
    "        target = target.to(DEVICE).double() #send target to GPU if available and set the data in double precision\n",
    "\n",
    "        ## forward pass\n",
    "        scores = model(data) #calculate the steering angle prediction\n",
    "        loss = torch.sqrt(criterion(scores, target)) #calculate the MSE\n",
    "\n",
    "        optimizer.zero_grad() #to avoid accumulating gradients of previous batches\n",
    "        loss.backward() #backpropagation\n",
    "\n",
    "        #update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        # record training loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "    model.eval() # prep model for evaluation\n",
    "    for data, target in valid_dataloader: #iterating over each validation batch after an epoch\n",
    "        data = data.to(DEVICE).double() #send data to GPU if available and set the data in double precision\n",
    "        target = target.to(DEVICE).double() #send target to GPU if available and set the data in double precision\n",
    "        \n",
    "        # forward pass: compute predicted steering angles by passing inputs to the model\n",
    "        output = model(data)\n",
    "        # calculate the loss\n",
    "        loss = torch.sqrt(criterion(output, target))\n",
    "        # record validation loss\n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "    # print training/validation statistics \n",
    "    \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_losses) #mean of RMSE for each epoch in training set\n",
    "    valid_loss = np.average(valid_losses) #mean of RMSE for each epoch in validation set\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "\n",
    "    epoch_len = len(str(NUM_EPOCHS))\n",
    "    \n",
    "    #printing the training and validation loss after each epoch\n",
    "    print_msg = (f'[{e:>{epoch_len}}/{NUM_EPOCHS:>{epoch_len}}] ' +\n",
    "                  f'train_loss: {train_loss:.5f} ' +\n",
    "                  f'valid_loss: {valid_loss:.5f}')\n",
    "    \n",
    "    print(print_msg+'\\n')\n",
    "    \n",
    "    # clear lists to track next epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "    \n",
    "    # early_stopping needs the validation loss to check if it has decresed, \n",
    "    # and if it has, it will make a checkpoint of the current model\n",
    "    early_stopping(valid_loss, model)\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopped..\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: The training was stopped at patience 6 of early stopping becuase runtime expired, but the model was not improving so the retraining was not necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iinK_ZhmnoTK",
    "outputId": "32aee623-9adc-4626-cf1b-0566ad6e88fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load the weights of the best model which gave the minimum error\n",
    "model.load_state_dict(torch.load('checkpoint.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "c2ZTUxisb0C2"
   },
   "outputs": [],
   "source": [
    "test_losses = [] #to store RMSE of each batch in testing set\n",
    "model.eval() # prep model for evaluation\n",
    "for data, target in test_dataloader: #iterating over each batch in test set\n",
    "    data = data.to(DEVICE).double() #send data to GPU if available and set the data in double precision\n",
    "    target = target.to(DEVICE).double() #send target to GPU if available and set the data in double precision\n",
    "    \n",
    "    # forward pass: compute predicted steering angles by passing inputs to the model\n",
    "    output = model(data)\n",
    "    # calculate the loss\n",
    "    loss = torch.sqrt(criterion(output, target))\n",
    "    # record validation loss\n",
    "    test_losses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "txT-nuPMf4ux",
    "outputId": "be3ab306-43ed-4525-e85d-7f984c225adc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The RMSE of test set is 15.169\n"
     ]
    }
   ],
   "source": [
    "#to get a single value of RMSE\n",
    "test_loss = np.mean(test_losses)\n",
    "print(f'The RMSE of test set is {test_loss:.03f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9YXegPrUgJDz"
   },
   "source": [
    "References: \n",
    "1. https://towardsdatascience.com/how-to-create-a-gif-from-matplotlib-plots-in-python-6bec6c0c952c\n",
    "2. https://github.com/aladdinpersson/Machine-Learning-Collection/blob/master/ML/algorithms/neuralnetwork/NN.py\n",
    "3. https://panjeh.medium.com/scikit-learn-hyperparameter-optimization-for-mlpclassifier-4d670413042b\n",
    "4. https://github.com/Bjarten/early-stopping-pytorch"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
